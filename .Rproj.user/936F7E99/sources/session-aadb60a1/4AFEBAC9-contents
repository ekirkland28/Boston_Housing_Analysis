---
title: 'STAT 3215Q: Final Project'
author: "Eleanor Kirkland"
date: "05/09/2025"
output:
  pdf_document: null
  highlight: default
  keep_tex: no
  fig_caption: yes
  latex_engine: pdflatex
  html_document:
    df_print: paged
affiliation: Department of Statistics, UConn
fontsize: 11pt
geometry: margin=1in
---
```{r Packages,echo=FALSE,message=FALSE}
library(psych)
library(nortest)
library(car)
library(tidyverse)
library(caret)
library(ncvreg)
library(mgcv)
library(gam)
library(splines)
library(gam)
library(glmnet)
library(ggplot2)
library(dplyr)
library(patchwork)
```

```{r Data,echo=FALSE}
Boston <- read.csv("/Users/elliekirkland/Desktop/HousingData.csv")
Housing <- subset(Boston, select = -c(B))
Housing <- na.omit(Housing)
# Rename columns
names(Housing)[1] <- "crime"
names(Housing)[2] <- "zoned"
names(Housing)[3] <- "ind_acres"
names(Housing)[4] <- "river"
Housing$river <- as.factor(Housing$river)
Housing$river <- relevel(Housing$river, ref="0")
names(Housing)[5] <- "nox"
names(Housing)[6] <- "rooms"
names(Housing)[7] <- "age"
names(Housing)[8] <- "distance"
names(Housing)[9] <- "highway"
names(Housing)[10] <- "tax_rate"
names(Housing)[11] <- "pt_ratio"
names(Housing)[12] <- "status"
names(Housing)[13] <- "price"
```

# 1. Introduction

> > This regression analysis sought to develop the best model for predicting the median house value in a particular area of Boston. To do so, a data set derived from information collected by the U.S. Census Service in 1970 concerning housing in the Boston, Massachusetts area was used. The information was originally for census tracts in the Boston Standard Metropolitan Statistical Area (SMSA). The data set contains 506 entries, each with 13 attributes. Its features are outlined below:

**Response variable:**

* `price`: median value of owner-occupied homes in the census tract in thousands of dollars

**Structural variables:**

* `rooms`: average number of rooms
* `age`: proportion of owner units built prior to 1940

**Neighborhood variables:**

* `status`: logarithmic proportion of population that is lower status
* `crime`: crime rate by town
* `zoned`: proportion of town's residential land zoned for lots greater than 25,000 square feet
* `ind_acres`: proportion nonretail business acres per town
* `tax_rate`: full value property tax rate
* `pt_ratio`: pupil-teacher ratio by town school district
* `river`: Charles River dummy (= 1 if census tract bounds Charles River)

**Accessibility variables:**

* `distance`: logarithm of weighted distances to five employment centers in Boston region
* `highway`: logarithm of index of accessibility to radial highways

**Pollution variable:**

* `nox`: nitrogen oxide concentrations in pphm (annual average concentration in parts per hundred million)

\newpage

```{r Censored Data, echo=FALSE}
Housing <- Housing[!(Housing$price >= 48 & Housing$price <= 52), ]
```

# 2. Methodology

## 2A. Exploratory Analysis

> Exploratory analysis gave insight into how the predictors are related to the response, thus guiding the more rigorous regression analysis.

The correlation matrix of the data set was first examined.
```{r Pairwise, echo=FALSE, fig.height=6, fig.width=6, fig.align="center"}
pairs.panels(Housing, ellipses=FALSE, hist.col = "orange",
method = "pearson", density = TRUE)
```
From this matrix, it was observed that the proportion of low status people (`status`) and the number of rooms (`rooms`) have the highest correlation with the median house value (`price`). This indicated that these two predictors may be most important in the prediction of median house value. 

\newpage

The marginal relationships between these two important predictors and median house value were be examined:
```{r Marginal Relationships, echo=FALSE, message=FALSE, fig.height=5, fig.width=6, fig.align="center"}
# status
status.plot <- ggplot(Housing, aes(x = status, y = price)) +
  geom_point(color = "black", size = 1) +  
  geom_smooth(method = "lm", se = FALSE, color = "orange", linewidth = 1) +
  labs(
    x = "Proportion Low Status", 
    y = "Median House Value (in 1000s)",
    title = "Median House Value by Proportion of Low Status Individuals"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face="bold")
  )

# rooms
rooms.plot <- ggplot(Housing, aes(x = rooms, y = price)) +
  geom_point(color = "black", size = 1) +  
  geom_smooth(method = "lm", se = FALSE, color = "orange", linewidth = 1) +
  labs(
    x = "Number of Rooms", 
    y = "Median House Value (in 1000s)",
    title = "Median House Value by Number of Rooms"
  ) +
  theme(
    plot.title = element_text(hjust = 0.5, face="bold")
  )

marginal.plot = status.plot / rooms.plot
marginal.plot
```
From these marginal plots, it appeared the proportion of low status individuals (`status`) was negatively correlated with the median house value (`price`), and it appeared the number of rooms (`rooms`) was positive correlated with the median house value (`price`). These relationships were explored further with regression analysis.

\newpage

## 2B. Regression Methods

```{r Training/Testing Data, echo=FALSE}
set.seed(124)
trainIndex <- createDataPartition(Housing$price, p=0.7, list=FALSE)
Housing.train <- Housing[trainIndex, ]
Housing.test <- Housing[-trainIndex, ]
```

### Simple Linear Regression

A simple linear regression model to predict median house value (`house_price`) was first constructed using proportion low status (`status`) as the single predictor.
$$E(\text{price}|X=x) = \beta_0 + \beta_1\text{status}$$

```{r SLR (status),echo=FALSE,results='hide'}
slr.status <- lm(price ~ status, data=Housing.train)
summary(slr.status)
```
The t-test on $\beta_1$ yielded a rejection of the null hypothesis $\beta_1=0$. Therefore, `status` provides useful information for predicting `price`. The $R_2$ value is 0.5677, meaning variability in `status` explains 57 percent of the variability in `price`. 

A second simple linear regression model was fit using number of rooms (`rooms`) as the single predictor.
$$E(\text{price}|X=x) = \beta_0 + \beta_2\text{rooms}$$
```{r SLR (rooms),echo=FALSE,results='hide'}
slr.rooms <- lm(price ~ rooms, data=Housing.train)
summary(slr.rooms)
```
The t-test on $\beta_2$ yielded a rejection of the null hypothesis $\beta_2=0$. Therefore, the number of rooms (`rooms`) provides useful information for predicting the median house value (`house_value`). The $R_2$ value is 0.5654, meaning variability in `rooms` explains 57 percent of the variability in `price`.  

In an effort to obtain a model that explains more of the variation in median house value (`price`) and accounts for interactions between predictors, a multiple linear regression model was constructed to predict `price`.

### Multiple Linear Regression

A multiple linear regression model to predict `price` was constructed using `status`, `rooms`, `pt_ratio`, `tax_rate`, `highway`, `distance`, `age`, `nox`, `river`, `ind_acres`, `zoned`, and `crime` as predictors.
```{r MLR,echo=FALSE}
set.seed(476)
mlr <- lm(price ~ ., data=Housing.train)
summary(mlr)
```
The variability in the included predictors now accounted for 80 percent of the variability in `price`. This is a significant improvement from the initial simple linear regression models.

The **Overall F Test** for this multiple linear regression model was also considered. An Overall F Test determines if there is a regression relation between median house value (`price`) and the set of all regressors. In other words, it determines if the model is useful in predicting the median house value. The hypotheses for this test are as follows:
$$H_0: \beta_1 = \cdots = \beta_{12} = 0$$
$$H_A: \text{at least one }\beta_j \ne 0\text{ , j}=1,\dots,12$$
The p-value associated with this Overall F Test was very small, so the null hypothesis ($H_0$) was rejected in favor of the alternative hypothesis ($H_A$). There is a regression relationship between median house value and the set of regressors.

Given that proportion of low status (`status`) and number of rooms (`rooms`) were most highly correlated with the median house value (`price`), a **General F Test** that tests the contribution of all remaining predictors to explaining the variability in median house value, given `status` and `rooms` are already included in the model was considered. The hypotheses for this test are as follows:

$$H_0: \beta_3 = \cdots = \beta_{12} = 0$$
$$H_A: \text{at least one }\beta_j \ne 0\text{ , j}=3,\dots,12$$
```{r General F Test 1, echo=FALSE}
mlr.red1 <- lm(price ~ status + rooms, data=Housing.train)
anova(mlr.red1, mlr)
```
The p-value associated with this General F Test was very small, so the null hypothesis ($H_0$) was rejected in favor of the alternative hypothesis ($H_A$). At least one of the partial slopes corresponding to the remaining predictors is not equal to 0. Furthermore, the remaining predictors significantly improve the prediction of median house value achieved from proportion of low status and number of rooms. 

The individual t-Tests indicate the non-significant regressors are `ind_acres` (proportion of nonretail business acres per town), `river` (proximity to Charles River), and `age`(proportion of owner units built preior to 1940). Another **General F Test** was conducted to test the contribution of `ind_acres`, `river`, and `age` to explaining the variability in median house value, given all other predictors are already included in the model. The hypotheses for this test are as follows[^1]:
$$H_0: \beta_3 = \beta_4 = \beta_7 = 0$$
$$H_A: \text{at least one }\beta_j \ne 0\text{ , j}=3,4,7$$
```{r General F Test 2, echo=FALSE}
mlr.full <- lm(price ~ status + rooms + pt_ratio + tax_rate + highway + distance + nox + zoned + crime + ind_acres + river + age, data=Housing.train)
mlr.red2 <- lm(price ~ status + rooms + pt_ratio + tax_rate + highway + distance + nox + zoned + crime, data=Housing.train)
anova(mlr.red2, mlr.full)
```
The p-value associated with this General F Test was quite large, so the null hypothesis ($H_0$) was not rejected. The partial slopes corresponding to `ind_acres`, `river`, and `age` do not differ from 0. In other words, `ind_acres`, `river`, and `age` do not significantly improve the prediction of median house value achieved from the predictors already included in the model. Thus, these three predictors were removed from the model and only 9 regressors were considered in the prediction of median house value.

```{r New MLR, echo=FALSE}
mlr <- lm(price ~ status + rooms + pt_ratio + tax_rate
          + highway + distance + nox + zoned + crime,
          data=Housing.train)
```

\newpage

## 2C. Regression Diagnostics

An **overall Outlier Test with Bonferroni Correction** was conducted in order to determine if the data set contains any outliers.
```{r Outlier Test, echo=FALSE}
outlierTest(mlr, cutoff = 0.05)
```
The Outlier Test identified one observation as an outlier. The multiple linear regression was conducted without this observation to determine if the observation significantly affected the previous findings. From examining the coefficients of the predictors and the residual standard errors, it was determined that the inclusion of the outlier does not affect the significance of individual regressors or the significance of the overall model. Therefore, the outlier remained in the data set.
```{r MLR without Outliers, echo=FALSE, results='hide'}
Housing.outlier <- Housing.train[-c(366), ]
mlr.outlier <- lm(price ~ status + rooms + pt_ratio + tax_rate + highway + distance + nox + zoned + crime, data=Housing.outlier)
summary(mlr.outlier)
summary(mlr)
```

**Collinearity** between regressors was investigated by examining the regressors' variance inflation factor (VIF) values. 
```{r VIF, echo=FALSE}
vif(mlr)
```
None of the predictors had a VIF value greater than or equal to 10. Therefore, none of the predictors are highly correlated with other predictor variables. 

The **QQ-plot** and **histogram** were examined to investigate any non-normality of the errors.
```{r Normality Plots, echo=FALSE, fig.height=3.5, fig.width=4.75, fig.align='center'}
ti <- rstudent(mlr)
par(mfrow=c(1,2))
qqnorm(ti); qqline(ti)
m<-mean(ti); std<-sd(ti)
hist(ti, density=20, breaks=20, prob=TRUE, xlab="Studentized Residuals",
     main="Histogram")
curve(dnorm(x, mean=m, sd=std), col="darkblue", lwd=2, add=TRUE, yaxt="n")
par(mfrow = c(1,1))
```
From the QQ-plot, it was observed the residuals may have a right skew.

The **Shapiro-Wilk normality test** and **Anderson-Darling normality test** were conducted to determine if the normality of errors assumption holds. 
```{r Normality Test, echo=FALSE}
shapiro.test(ti)
ad.test(ti)
```
Since the p-value for each of these tests was very small, the null hypothesis was rejected; the errors do not follow a normal distribution. 

The **Breuch-Pagan (BP) Test** was conducted in order to evaluate the assumption of the errors having equal variances. 
```{r Heteroscedasticity Test, echo=FALSE}
ncvTest(mlr)
```
Since the p-value for this test was large, the null hypothesis was not rejected; the errors had a constant variance. 

**Tukey's Test for curvature/nonadditivity** was conducted to determine if any of the regressors have a non-linear relationship with median house value (`price`).
```{r Tukey Test, echo=FALSE, fig.show = "hide"}
residualPlots(mlr, type="rstudent")
```
The p-values associated with the Tukey Tests for `status`, `rooms` and `tax_rate` were smaller than an $\alpha=0.05$ significance level. Therefore, the relationship between each of these predictors and the median house value has a significant non-linear component. This suggested the inclusion of higher order terms of these regressors was warranted. Additionally, the p-value associated with the Tukey Test for nonadditivity was very small, which suggested the linear mean function specified in the multiple linear regression model is not sufficient.

\newpage

## 2D. Regression Remedies

Including quadratic terms for `status`, `rooms`, and `tax_rate` was thought to remedy the linearity assumption. To do so, the multiple linear regression was fit with **orthogonal polynomials** that were functions of the original predictors. 
```{r Orthogonal Polynomials, echo=FALSE, results='hide', fig.align='center', fig.width=3.75, fig.height=2.75}
mlr.orth <- lm(price ~ poly(status,degree=2,raw=FALSE) +
               poly(rooms,degree=2,raw=FALSE) +
               pt_ratio + poly(tax_rate,degree=2,raw=FALSE) + highway +
                 distance + nox + river + zoned +
                 crime, data=Housing.train)
residualPlot(mlr.orth, variable = "poly(status,degree=2,raw=FALSE)", type = "rstudent", xlab='Status as Polynomial')
residualPlot(mlr.orth, variable = " poly(rooms,degree=2,raw=FALSE)", type = "rstudent", xlab='Rooms as Polynomial')
residualPlot(mlr.orth, variable = " poly(tax_rate,degree=2,raw=FALSE)", type = "rstudent", xlab='Tax Rate as Polynomial')
```
The residual plots for `status`, `rooms`, and `tax_rate` now showed little curvature, indicating the linearity assumption seemed to be more satisfied.

\newpage

In an attempt to address the non-normality of the errors, a **Box-Cox transformation** was applied to the `price`. However, this did not remedy the non-normality. A **bootstrap case resampling** was conducted to determine if the non-normality of the errors was severely affected the estimates of the regression coefficients. The confidence intervals of the bootstrap regression coeffients were much wider than those of the original model, so it appeared the analysis were affected by the non-normality.  
```{r Bootstrap, echo=FALSE, results='hide', message=FALSE}
mlr.boot <- Boot(mlr, method="case")
summary(mlr.boot)
```
Finally, it was observed that many observations had a median house value of exactly 50 and that these observations were skewing the distribution of the errors. These values were removed from the data set in an attempt to make the distribution more normal. While the null hypothesis of the Shapiro-Wilk normality test was still rejected, the p-value was less small and the QQ-plot showed a more normal distribution. This was deemed sufficient because this model was being constructed to *predict* median house values in Boston neighborhoods (rather than to make statistical inferences), so it was not imperative that median house values followed a normal distribution. 

\newpage

## 2E. Model/Variable Selection

The **full model** obtained from the previous sections was compared to candidate submodels. To clarify, the full model is `price ~ f(status) + f(rooms) + pt_ratio + f(tax_rate) + highway + distance + nox + river + zoned + crime`, where `f(status)`, `f(rooms)`, `f(pt_ratio)`, and `f(tax_rate)` are orthogonal functions of the corresponding original predictors.

To best assess validity of each candidate submodel, the data was partitioned into a training subset and a testing subset. These training and testing subsets were used in two variable selection techniques. The models obtained from these techniques were compared using an out-of sample metric.
```{r Split Data, echo=FALSE}
X.train <- model.matrix(price ~ poly(status,degree=2,raw=FALSE) +
               poly(rooms,degree=2,raw=FALSE) +
               pt_ratio + poly(tax_rate,degree=2,raw=FALSE) + highway +
                 distance + nox + river + zoned +
                 crime, data=Housing.train)[, -1]
price.train <- Housing.train$price
X.test <- model.matrix(price ~ poly(status,degree=2,raw=FALSE) +
               poly(rooms,degree=2,raw=FALSE) +
               pt_ratio + poly(tax_rate,degree=2,raw=FALSE) + highway +
                 distance + nox + river + zoned +
                 crime, data=Housing.test)[, -1]
price.test <- Housing.test$price
```

First, an **Elastic Net regularization** was performed. Cross validation was performed to select the optimal tuning parameter $\lambda$.
```{r Elastic Net, echo=FALSE}
set.seed(123)
net <- glmnet(X.train, price.train, family='gaussian', alpha=0.5)
cv_net <- cv.glmnet(X.train, price.train, nfolds = 10)
coef(cv_net)
```
The Elastic Net regularization selected proportion low status (`status`), number of rooms (`rooms`), pupil to teacher ratio (`pt_ratio`), property tax rate (`tax_rate`), nitrogen oxide concetrations (`nox`), and crime rate by town (`crime`) as the set of active predictors. It selected the quadratic terms of `status`, `rooms`, and `tax_rate`.

\newpage

Next, **SCAD** was implemented. 
```{r SCAD, echo=FALSE}
set.seed(123)
cv_scad <- cv.ncvreg(X.train, price.train, nfolds=10, penalty="SCAD")
coef(cv_scad)
```
SCAD did not remove any predictors from the set of active predictors. 

The **mean squared prediction error (MSE)** values of the full model, the Elastic Net model, and the SCAD model were compared in order to assess the predictive ability of each model.
```{r MSE Calculation, echo=FALSE, results='hide'}
y_test_hat.orig <- as.numeric(predict(mlr.orth, newdata = Housing.test))
MSE.orig <- mean((price.test - y_test_hat.orig)^2)
MSE.orig

y_test_hat.net <- as.numeric(predict(cv_net, newx=X.test), type="response")
MSE.net <- mean((price.test - y_test_hat.net)^2)
MSE.net

y_test_hat.scad <- as.numeric( predict(cv_scad, X=X.test), type="response")
MSE.scad <- mean((price.test - y_test_hat.scad)^2)
MSE.scad
```

```{r MSE Results, echo=FALSE}
return <- c(MSE.orig, MSE.net, MSE.scad)
names(return) <- c("Original", "Net", "SCAD")
return
```

The full model constructed before performing any variable selection had the lowest MSE value. This indicated the full model performs best at predicting median house value (`price`) based on new predictor values. Elastic Net and SCAD may have too aggressively shrunk important predictors to zero. Those regularization methods did provide more simple models, but given the purpose of this analysis is to develop a model with the highest prediction accuracy, the full model is favored over the simpler models.

\newpage

Plots of each model's predictions of house value versus the actual house values were observed to confirm the selection of the original model.
```{r Predicted vs. Actual Plot, echo=FALSE, fig.align='center', fig.width=3.75, fig.height=2.75}
predicted.orig <- (predict(mlr.orth, newdata = Housing.test))
plot(predicted.orig, price.test,
     xlab = "Predicted House Value",
     ylab = "Actual House Value",
     main = "Accuracy of Original Model",
     pch = 21,          
     bg = "steelblue",  
     col = "black",     
     cex = 1.2)        
abline(0, 1, col = "orange", lwd = 2)

predicted.net <- as.numeric(predict(cv_net, newx=X.test), type="response")
plot(predicted.net, price.test,
     xlab = "Predicted House Value",
     ylab = "Actual House Value",
     main = "Accuracy of Elastic Net Model",
     pch = 21,          
     bg = "steelblue",  
     col = "black",     
     cex = 1.2)        
abline(0, 1, col = "orange", lwd = 2)

predicted.scad <- as.numeric( predict(cv_scad, X=X.test), type="response")
plot(predicted.scad, price.test,
     xlab = "Predicted House Value",
     ylab = "Actual House Value",
     main = "Accuracy of SCAD Model",
     pch = 21,          
     bg = "steelblue",  
     col = "black",     
     cex = 1.2)        
abline(0, 1, col = "orange", lwd = 2)
```
It was observed the strongest linear relationship between predicted values and actual values was in the original model's plot. This confirmed the selection of the original model as the most suitable model for prediction of median house values.

\newpage

## 2F. Statistical Tests

The statistical tests performed throughout the analysis are summarized again here for completeness.

### Regression Methods

> In the Simple Linear Regression section, an **individual t-test** on the coefficient of `status` was conducted. This test determined whether the coefficient of `status` was different from zero. The null hypothesis that the coefficient is zero was rejected, indicating `status` provides useful information for predicting `price`. Another individual t-test was conducted on the coefficient of `rooms`. The null hypothesis for this test was rejected, indicating `rooms` provides useful information for predicting `price`. These tests determined two predictors individually provide useful information for predicting median house value.
> In the Multiple Linear Regression section, an **Overall F Test** of the multiple linear regression model was conducted. The p-value associated with this test was very small, so the null hypothesis that all of the regression coefficients are zero was rejected. This test determined there was a regression relationship between median houes value and the set of regressors. Therefore, it determined the predictors included in the dataset provide useful information for the prediction of median house value. A **General F Test** was conducted to test the contribution of all remaining predictors to explaining the variability in `price`, given `status` and `rooms` are already included in the model. The null hypothesis was rejected, indicating at least one of the partial slopes corresponding to the remaining predictors was not equal to 0. This test demonstrated that the remaining predictors should be included in the model for predicting median house value. Another **General F Test** was conducted to test the contribution of `ind_acres` and `age` to explaining the variabiltiy in `price`, given all other predictors are already included in the model. The null hypothesis of this test was not rejected, indicating the partial slopes corresponding to `ind_acres` and `age` did not differ from 0. This test demonstrated that `ind_acres` and `age` were not important in the prediction of median house value.  

### Regression Diagnostics

> An **overall Outlier Test with Bonferroni Correction** was conducted to determine if the data set contained any outliers. One data point was determined to be an outlier, as it had a very small Bonferroni p-values. This test revealed a point that could have influenced the building and analysis of the model. Two **normality tests** were conducted to determine if the normality of errors assumption held. The null hypothoses of these tests were rejected, indicating the errors did not follow a normal distribution. This test indicated an assumption of the model was violated. The **Breuch-Pagan (BP) Test** was conducted to evaluate the assumption of the errors having constant variance. The null hypothesis of this test was not rejected, indicating the errors had a constant variance. This test demonstrated an assumption of the model was violated. **Tukey's Test** was conducted for each predictor. The null hypotheses associated with `status` `rooms`, and `tax_rate` were rejected, indicating these predictors have a non-linear relationship with the response `price`. This test revealed a better prediction of median house value could be achieved by including higher-order terms. 

\newpage

# 3. Conclusion

> This analysis determined that the following predictors are useful in the prediction of median house value: proportion low status, average number of rooms, pupil-teacher ratio, property tax rate, accessibility to radial highways, distance to employment centers, nitrogen oxide concentrations, proprtion of residential land zoned for large lots, and crime rate. This analysis also determined proportion low status, number of rooms, and property tax rate have a nonlinear relationship with median house value. This suggests median house value is impacted differently depending on the values of these predictors. This analysis captured these non-linear relationships by developing a linear model with higher-order terms. This model was able to accurately predict median house values given unseen predictor values. This model could be used by economists and policymakers to predict how proposed changes to any of the predictors would affect the median house value in a given area of Boston. For instance, if policymakers were to propose a new property tax rate for Dorchester, this tax rate could be fed to the model in order to determine how the median house value in Dorchester would be affected by this change in tax rate. In general, this linear regression model is a useful tool in predicting how changes to the makeup of Boston neighborhoods will affect the value of homes in those neighborhoods.

[^1]: Note the order of the predictors was adjusted in order to conduct this General F Test.